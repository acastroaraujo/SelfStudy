---
title: "Statistical Modeling with RStanarm"
output: 
  html_document:
    code_folding: show
    theme: paper
    toc: yes
    toc_float: 
      collapsed: no
---

```{css, include=FALSE}
pre code, pre, code {
  white-space: pre !important;
  overflow-x: scroll !important;
  word-break: keep-all !important;
  word-wrap: initial !important;
}
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align = "center",
                      fig.width = 5, fig.height = 4,
                      comment = "", 
                      cache = TRUE)

## Packages
library(tidyverse)
library(rstanarm)
options(mc.cores = parallel::detectCores())

## Plot settings
theme_set(
  theme_minimal(base_family = "Avenir")
  )
```

<font size="2"> See [___here___](http://mc-stan.org/rstanarm/) for the official documentation. <br> See [___here___](https://mc-stan.org/users/documentation/case-studies/pool-binary-trials-rstanarm.html) for a complete case study.</font> 

## Introduction

- In frequentist inference, we care more about the probability of observing some random sample, given some fixed parameters.

$$
p(\mathbf y \mid \boldsymbol \theta)
$$

- In Bayesian inference, we care more about the probability of parameters, given some fixed data.

    $$p(\boldsymbol \theta \mid \mathbf y)$$

    Furthermore, Bayesian inferences include prior information to get more stable estimates and predictions, or to account for additional structure in the data. And uncertainty is characterized using simulations from probability distributions (instead of relying on standard errors or bootstrap approximations).

****

### Hello World (Sampling)

```{r, echo=FALSE}
URL <- "http://www.stat.columbia.edu/~gelman/arm/examples/ElectionsEconomy/hibbs.dat"
hibbs <- read.table(URL, header = TRUE)
```

This is how the output of normal OLS regression will look like: 

```{r}
OLS <- lm(inc.party.vote ~ growth, data = hibbs)
OLS %>% 
  arm::display(digits = 2)
```

```{r}
hibbs %>% 
  ggplot(aes(growth, inc.party.vote)) +
  geom_text(aes(label = year)) +
  geom_smooth(method = lm, color = "steelblue1")
```

And this is how the output of the same regression, fitted through `rstanarm`, looks like:

```{r, results = "hide"}
b_mod <- stan_glm(inc.party.vote ~ growth, data = hibbs)
```
```{r}
b_mod %>% 
  summary(digits = 2)
```

The contents of this object include things that are usually unavailable for regular `lm()` or `glm()` objects. For example:

```{r}
draws <- as_tibble(b_mod)

draws %>% 
  gather(key = parameters, value = draw) %>% 
  group_by(parameters) %>% 
  summarise(
    Median = median(draw),
    MAD_SD = mad(draw)
    )


g <- draws %>%
  rename(intercept = `(Intercept)`) %>% 
  ggplot(aes(intercept, growth)) +
  geom_point(alpha = 0.2)

g
```

```{r}
b_mod$coefficients
b_mod$covmat
```

```{r}
df <- crossing(  ## parameter grid
  a = seq(40, 52, length.out = 100), 
  b = seq(0, 6, length.out = 100)) %>% 
  mutate(  ## joint density
    dens = map2(a, b, cbind) %>% 
      map_dbl(mvtnorm::dmvnorm, 
              mean = b_mod$coefficients,
              sigma = b_mod$covmat)
    )

g + geom_contour(data = df, 
                 aes(a, b, z = dens),
                 color = "steelblue1", bins = 10, size = 1) 
      
```

```{r, fig.width=8, fig.height=3, message=FALSE}
draws %>% 
  rename(intercept = `(Intercept)`) %>% 
  bayesplot::mcmc_dens(
    pars = c("intercept", "growth", "sigma"),
    facet_args = list(nrow = 1))

draws %>% 
  gather(key = pars, value = draw) %>% 
  ggplot(aes(x = draw)) +
  geom_density(fill = "steelblue1", alpha = 0.5) +
  facet_wrap(~pars, scales = "free")

draws %>% 
  gather(key = pars, value = draw) %>% 
  ggplot(aes(x = pars, y = draw)) +
  geom_jitter(alpha = 0.1) +
  geom_boxplot(outlier.color = NA) +
  facet_wrap(~pars, scales = "free")
```

```{r}
b_mod$algorithm
```

```{r, fig.width=8, fig.height=2, message=FALSE}
ggplot(NULL, aes(x = b_mod$stanfit@sim$samples[[1]]$mean_PPD)) +
  geom_histogram(fill = "steelblue1", color = "black") +
  geom_vline(xintercept = mean(hibbs$inc.party.vote), size = 3, color = "tomato") +
  labs(x = "mean_PPD")
```

**Note**.

- `posterior_linpred()` yields simulations of the possible values of $\boldsymbol \eta = \alpha + \beta \mathbf x$, with variation coming from the posterior uncertainty of the coefficients.

- `posterior_predict()` yields simulations of the possible values of $\mathbf y^\text{rep} = \alpha + \beta \mathbf x + \epsilon$. This is also known as the _posterior predictive distribution_ of the data.

```{r, fig.width=8, fig.height=2, message=FALSE}
bayesplot::ppc_stat(y = hibbs$inc.party.vote, 
                    yrep = posterior_predict(b_mod), stat = mean)
```

The `posterior_predict()` function is simply a short hand for the following procedure:

```{r}
draws <- draws %>% 
  rename(a = `(Intercept)`, b = growth)

n_sims <- nrow(draws)
n_obs <- nrow(hibbs)

yrep <- array(NA, dim = c(n_sims, n_obs))
for (i in 1:n_obs) {
  yrep[ , i] <- rnorm(
    n = n_sims, 
    mean = draws$a + draws$b * hibbs$growth[i], 
    sd = draws$sigma)
}

dim(yrep)
dim(posterior_predict(b_mod))
```

```{r, fig.width=8, fig.height=2, message=FALSE}
ggplot(NULL, aes(x = apply(yrep, 1, mean))) +
  geom_histogram(color = "black", fill = "steelblue1") +
  geom_vline(xintercept = mean(hibbs$inc.party.vote), 
             size = 3, color = "tomato") +
  labs(x = "mean_PPD")
```

```{r}
hibbs %>% 
  ggplot(aes(growth, inc.party.vote)) +
  geom_abline(
    data = draws,
    mapping = aes(intercept = a, slope = b),
    alpha = 0.01, color = "steelblue1") +
  geom_abline(
    intercept = b_mod$coefficients[1],
    slope = b_mod$coefficients[2],
    linetype = "dashed") +
  geom_point()
```

#### Making Forecasts

Suppose we are interested in predicting the election outcomes when growth is 2%. We simply do as follows:

```{r}
new_df <- tibble(growth = 2)
ypred <- posterior_predict(b_mod, newdata = new_df)

cat("Predicted percentage of 2-party vote: ", 
    round(median(ypred), 2), ", with s.e. ", 
    round(sd(ypred), 2), "\n\nPr(Incumbent Win) = ", 
    round(mean(ypred > 50), 2), sep = "") 
```

Then again, it's hard to believe a 2% growth estimate without any uncertainty attached to it.

Suppose that our best estimate of economic growth was 2%, but with some uncertainty that will be expressed as a normal distribution with standard deviation 0.3%. We can then _propagate the uncertainty_ in this predictor to obtain a forecast distribution that more completely expresses our uncertainty:

```{r}
draws <- draws %>% 
  mutate(x = rnorm(n_sims, mean = 2, sd = 0.3)) %>% 
  mutate(ypred = rnorm(n_sims, a + b*x, sigma))

with(draws,
cat("Predicted percentage of 2-party vote: ", 
    round(median(ypred), 2), ", with s.e. ", 
    round(sd(ypred), 2), "\n\nPr(Incumbent Win) = ", 
    round(mean(ypred > 50), 2), sep = ""))
```

Note that the standard error is slightly higher and thus, the probability of a win is slightly lower.

****

```{r}
prior_summary(b_mod)
```

See more information [here](http://mc-stan.org/rstanarm/reference/prior_summary.stanreg.html) and [here](http://mc-stan.org/rstanarm/reference/priors.html).

```{r, results="hide"}
g <- posterior_vs_prior(b_mod, 
                        group_by_parameter = TRUE,
                        facet_args = list(scales = "free"))
```
```{r, fig.width=8, fig.height=3}
g
```

Ultimately, these are very generic weakly informative priors. If we want to exert more control over them, we need to set up additional arguments, such as:

```
stan_glm(...
  prior_intercept = normal(0, 1, autoscale = FALSE),
  prior = student_t(df = 1, 0, 1, autoscale = FALSE),
  prior_aux = cauchy(autoscale = TRUE))
```

****

### Hello World (MAP)

The following example gets rid of this quantity by setting `mean_PPD = FALSE`. It also sets uniform prior for all coefficients, and uses the "optimizing" algorithm instead of "sampling". 

The "optimizing" algorithm produces a MAP estimate or a _normal approximation centered at the posterior mode._ The idea is to take the Bayesian posterior distribution, and instead of sampling from it, which can be slow when the number of regression coefficients is large, we use an optimization algorithm to find its mode, and then use the curvature of the posterior density to construct a normal approximation.

This normal approximation is not perfect, especially for small datasets where there’s necessarily more uncertainty in the inference. But it can be used for speeding computations in "big data" applications.

```{r, results="hide"}
map_mod <- stan_glm(inc.party.vote ~ growth, data = hibbs,
                    prior = NULL,
                    prior_intercept = NULL,
                    prior_aux = NULL,
                    mean_PPD = FALSE, 
                    algorithm = "optimizing")
```
```{r}
map_mod %>% 
  summary(digits = 2)
```

```{r}
prior_summary(map_mod)
```

## Diagnostic plots

```{r, message=FALSE, warning=FALSE}
library(bayesplot)
bayesplot_theme_set(theme_minimal(base_family = "Avenir"))
color_scheme_set("viridis")

n_pars <- nuts_params(b_mod)
```

```{r}
rhat(b_mod)
```

```{r}
b_mod %>% 
  as.array() %>% 
  mcmc_trace(facet_args = list(ncol = 1),
             np = n_pars)
```

```{r}
b_mod %>% 
  as.array() %>% 
  mcmc_pairs(off_diag_args = list(size = 1, alpha = 0.2), 
             np = n_pars)
```

## Model Fit

### PPC

**Brief overview of Posterior Predictive Checks (PPCS)**

The intuition is simple: *if the model fits, then replicated data generated under the model should look similar to observed data.* And any systematic differences between the simulations and the data indicate potential failings of the model.

- Let $y$ be the observed data and $\boldsymbol \theta$ be the vector of parameters (including all the hyperparameters if the model is hierarchical).

- We define $y^\text{rep}$ as the replicated data that could have been observed, or, to think predictively, as the data we would see tomorrow if the "experiment" (or "process") that produced $y$ today were replicated with the same generative model and the same value of $\boldsymbol \theta$ that produced the observed data.

- We distinguish between $y^\text{rep}$ and $\widetilde y$, our general notation for predictive outcomes: $\widetilde y$ is any future observable value or vector of observable quantities, whereas $y^\text{rep}$ is *specifically a replication* just like $y$. For example, if the model has explanatory variables, $x$, they will be identical for $y$ and $y^\text{rep}$, but $\widetilde y$ may have its own explanatory variables, $\widetilde x$.

$$
p(\mathbf y^\text{rep} \mid \mathbf y) = \int p(\mathbf y^\text{rep} \mid \boldsymbol \theta) p(\boldsymbol \theta \mid \mathbf y)d\boldsymbol \theta
$$

- We define **test quantities** as the aspects of the data we wish to check (e.g. the *mean*, certain *quantiles*, outliers through *min* and *max*, and so forth). A test quantity (or *discrepancy measure*) $T(y, \theta)$ is a scalar summary of parameters *and* data used as a standard for comparing data to predictive simulations. Test quantities play the role in Bayesian model checking that test statistics play in classical testing. Thus, we use the notation $T(y)$ for classical *test statistics*.

```{r, fig.width=8, fig.height=2, message=FALSE}
color_scheme_set("blue")
ppc_stat(y = hibbs$inc.party.vote, 
         yrep = posterior_predict(b_mod), 
         stat = sd)

ppc_stat(y = hibbs$inc.party.vote, 
         yrep = posterior_predict(b_mod), 
         stat = max)

custom_test <- function(x) max(x) - min(x)

ppc_stat(y = hibbs$inc.party.vote, 
         yrep = posterior_predict(b_mod), 
         stat = custom_test)
```

```{r, fig.width=8, fig.height=2, message=FALSE}
samp <- sample(4000, 500)
ppc_dens_overlay(y = hibbs$inc.party.vote, 
                 yrep = posterior_predict(b_mod)[samp, ])

ppc_intervals(y = hibbs$inc.party.vote, 
                 yrep = posterior_predict(b_mod)) +
  scale_x_continuous(labels = hibbs$year, breaks = 1:16)
```

This last plot can be understood in terms of _calibration_. If, according to your model, $y$ should fall within a posterior predictive interval with probability 0.5, then around half of the observed $y$'s should do so.

```{r}
pp_interval_coverage <- function(y, yrep, lower, upper) {
  left <- apply(yrep, MARGIN = 2, quantile, probs = lower)
  right <- apply(yrep, MARGIN = 2, quantile, probs = upper)
  message(paste("Coverage of the", scales::percent(upper - lower), 
                 "posterior predictive interval:"))
  return(mean(left < y & y < right))
}

pp_interval_coverage(
  y = hibbs$inc.party.vote, 
  yrep = posterior_predict(b_mod),
  lower = 0.25,
  upper = 0.75)

pp_interval_coverage(
  y = hibbs$inc.party.vote, 
  yrep = posterior_predict(b_mod),
  lower = 0.05,
  upper = 0.95)
```

These numerical assessments of calibration will give you a _rough_ idea of how the model underfits (or overfits). Ideally, though, you'd want to have more than 100 data points for these percentages to make sense.

### $R^2$

One of the most common measures of model fit in linear regression is the $R^2$ statistic. This is also known as the _coefficient of determination_.

For example, a model with $R^2 = 0.7$ "explains" 70% of the _in-sample variance_ of $y$. The remaining 30% is the amount of variation not predicted by the model.

The following table provides a brief summary of the concepts embedded in the $R^2$ statistic:

| Concept                      | Notation
|:-----------------------------|:-------------------------------------|
| Predicted outcomes (or *fitted* values) | $\hat y_i = \hat \alpha + \hat \beta x_i$
| Prediction error (or *residual*)        | $\hat \epsilon_i = y_i - \hat y_i$
| Total sum of squares (TSS)              | $\text{TSS} = \sum_{i = 1}^n (y_i - \bar y)^2$
| Sum of squared residuals (SSR)          | $\text{SSR} = \sum_{i=1}^n \hat\epsilon_i^2$
| Coefficient of determination ($R^2$)    | $R^2 = \frac{\text{TSS} - \text{SSR}}{\text{TSS}} = 1 - \frac{\text{SSR}}{\text{TSS}}$

Alternatively, we can think of $R^2$ as a quantity that shows how much of the variation in $y$ is reduced by the model.

$$
\text{TSS}(1- R^2) = \text{SSR}
$$

Going back to the OLS example:

```{r}
OLS %>% 
  arm::display()
```

```{r}
SSR <- var(residuals(OLS))
TSS <- var(predict(OLS)) + SSR
1 - (SSR/TSS) ## R2
```

Alternatively, we can define $R^2$ as a simple ratio:

$$
R^2 =\frac{\text{var}(\hat y)}{\text{var}(y)} = \frac{\sum_{i=1}^n (\hat y - \bar y)^2}{\text{TSS}}
$$

```{r}
var(predict(OLS)) / TSS
```

Note the following things about $R^2$:

1. $R^2$ is bounded between zero and one.

2. It does not change if you multiply $x$ or $y$ by a constant. So, if you want to change the units of the predictors or response to aid interpretation, you won't change the summary of the fit of the model to the data.

3. In a regression with one predictor, one can show that $R^2$ equals the square of the linear correlation $\rho$ between $x$ and $y$.

4. Interpreting $R^2$ can get tricky because the numerator and denominator can be changed in different ways. For example, if you apply a model to a subset of the original data, then the TSS might be reduced, which causes $R^2$ to decrease as well. This can happen even if the model fits both data sets just as well.

**Going Bayesian**

$R^2$ is based on the point estimate of the fitted model. But in Bayesian inference we don't really do point estimates, instead we deal with posterior distributions. We can get at this through the `bayes_R2()` function in rstanarm.

```{r, fig.width=8, fig.height=2, message=FALSE}
NULL %>% 
  ggplot(aes(x = bayes_R2(b_mod))) +
  geom_histogram(color = "black", fill = "steelblue1")
```

Basically, we get a _vector_ of predicted values and residuals for each simulation draw.

```{r}
yhat_mat <- posterior_linpred(b_mod)
resid_mat <- array(NA, dim = dim(yhat_mat))

for (i in 1:4000) {
  resid_mat[i, ] <- hibbs$inc.party.vote - yhat_mat[i, ]
}

var_ypred <- apply(yhat_mat, MARGIN = 1, var)
var_resid <- apply(resid_mat, MARGIN = 1, var)

BR2 <- var_ypred / (var_ypred + var_resid)
```

```{r, fig.width=8, fig.height=2, message=FALSE}
NULL %>% 
  ggplot(aes(x = BR2)) +
  geom_histogram(color = "black", fill = "steelblue1")
```

From Gelman et al (2017):

- A new issue arises when fitting a set of a models to a single dataset. Now that the denominator of $R^2$ is no longer fixed, we can no longer interpret an increase in $R^2$ as a improved fit to a fixed target.

    The new denominator can be interpreted as an estimate of the _expected variance of predicted future data_ from the model _under the assumption that the predictors $\mathbf X$ are held fixed_. In other words we can consider our Bayesian $R^2$ as a data-based estimate of the proportion of variance explained for new data. 
    
    Note that the predictive accuracy for new data will be lower (in expectation) than the predictive accuracy for the data used to fit the model. That is, the $R^2$ statistic doesn't provide any correction for _overfitting_ and, as such, in the same way that is done for log-score measures via cross-validation.
    
**The problem with $R^2$**

We are usually not interested in whether or not a model can predict the data upon which it was trained. Rather, we usually care about _future data_ (or "testing" data). And here we face a trade-off between model complexity and predictive accuracy; or between _underfitting_ (models that learn too little from the data sample) and _overfitting_ (models that learn too much from the data sample). 

Usually, adding more parameters to a model will produce overfitting. We can even reach incredible $R^2$ quantities if the number of parameters approximates the number of observations. This is true even when the variables you add to a model are just random numbers, with no relation to the outcome. So it’s no good to choose among models using only fit to the data.

Note. There are exceptions to this "too many parameters" heuristic, such as multilevel models and other forms of regularization like "penalized likelihood estimation".

_Example of an improved $R^2$ through a nonsensical approach:_

```{r, echo=FALSE, fig.width=7, fig.height=4, warning=FALSE, message=FALSE}
poly_plot <- function(N) {
  OLS <- lm(inc.party.vote ~ poly(growth, N), data = hibbs)
  ggplot(hibbs, aes(y = inc.party.vote, x = growth)) +
  geom_point() + 
  geom_smooth(method = lm,
              formula = y ~ poly(x, N), col = "red"
              ) +
  labs(title = paste("R-Squared =", round(summary(OLS)$r.squared, 2)),
       subtitle = paste("Polynomial OLS, degree", N)) +
    coord_cartesian(xlim = c(0, 4.5), ylim = c(40, 65))
}


gridExtra::grid.arrange(
  poly_plot(2), poly_plot(5), 
  poly_plot(13), poly_plot(15)
  )
```

*Other problems with $R^2$*

Looking back at the ratio exposition of $R^2$ reveals at least two more problems:

$$
R^2 = \frac{\text{var}[\beta_0 + \beta_1 X]}{\text{var}[\beta_0 + \beta_1 X + \epsilon]} = \frac{\text{var}[X]}{\text{var}[X] + \sigma^2}
$$

1. _$R^2$ does_ not _measure goodness of fit_. By making the variance of the linear predictor arbitrarily small (or $\sigma^2$ arbitrarily large), we get $R^2 \leadsto 0$. And that will happen even if we are looking at the "true" model.

2. _$R^2$ says nothing about predictive accuracy_. This is because, depending on the range of $X$, the $R^2$ can be anywhere between 0 and 1. The following simulations show that the mean squared error is a much more reliable measure of predictive accuracy.

```{r}
mse <- function(OLS) {
  output <- mean((OLS$fitted.values - OLS$model$y)^2)
  return(output)
}

## Small range for X
set.seed(123)
x <- seq(1, 2, length.out = 1e3)
y <- rnorm(n = 1e3, mean = 1 + x*2, sd = 1)

OLS <- lm(y ~ x)
summary(OLS)$r.squared
mse(OLS)

## Big range for X
set.seed(123)
x <- seq(1, 10, length.out = 1e3)
y <- rnorm(n = 1e3, mean = 1 + x*2, sd = 1)

OLS <- lm(y ~ x)
summary(OLS)$r.squared
mse(OLS)
```

Finally, $R^2$ is commonly described as "the fraction of variance explained". But this statistical usage of the word "explain" has no connection whatsoever to anything that could sensibly be called an explanation. And none of the above problems are fixed by using the so-called "adjusted-$R^2$", which penalizes the $R^2$ in proportion to the number of parameters included in the model. 

****

**If $R^2$ is so useless, then why are we waisting so much time talking about it?**

There are three main reasons to discuss $R^2$:

- _Historical reasons_. The $R^2$ is taught extensively in the traditional statistics curriculum, so you need to know what it is in order to be a practitioner.

- _Transitioning to better tools_. We would like to use the $R^2$ (or "adjusted $R^2$") to perform important tasks, such as model comparisons. The log-likelihood measures (or _information criteria_) discussed later on provide better tools, and you never really know how useful they are until you understand the poverty of $R^2$

- _Bayesian priors_. The `stan_lm`, `stan_aov`, and `stan_polr` allow users to set a prior on $R^2$ in order to convey information about _all_ the parameters. From the **`rstanarm`** documentation:

    > "This prior hinges on prior beliefs about the location of $R^2$, _the proportion of variance in the outcome attributable to the predictors_, which has a Beta prior with first shape hyperparameter equal to half the number of predictors and second shape hyperparameter free. By specifying what to be the prior mode (the default), mean, median, or expected log of $R^2$, the second shape parameter for this Beta distribution is determined internally."
    
    In other words, it provides more regularization!


## Model Comparison

https://sci-hub.tw/10.1177/0081175018793654

THIS SECTION IS A BIT IFFY FOR THE MOMENT.


In this section we introduce other measures that try to approximate out-of-sample predictive accuracy, which automatically deals with the problem of overfitting. We focus on leave-one-out cross-validation (or [__`loo`__](https://mc-stan.org/loo/index.html)) because it's rapidly becoming the standard in Bayesian analysis. 


### TITLE

Functions of _misfit_ used in traditional statistical analysis and machine learning are almost always proportional to some log-likelihood; for example, the sum of squared errors (i.e. the numerator in $R^2$) is proportional to the logarithm of a Gaussian log-likehood; the log probability of success is proportional to likelihood of a Bernouilli model; and so on.

HERE:

https://youtu.be/GJT5D68abVY


**Brief overview of information criteria**

Information criteria serve the purpose of _scoring devices_  used to estimate predictive accuracy. (Note: Measures of predictive accuracy for probabilistic prediction are called "scoring rules"). Particularly, we are interested in how the model predictions deviate from future data (or _out-of-sample deviance_). 


INFORMATION CRITERIA

Sometimes we care about this accuracy for its own sake, as when evaluating a forecast. In other settings, predictive accuracy is valued for comparing different models rather than for its own sake

**The ideal measure**

The _ideal_ measure of model fit would be the _expected out-of-sample log predictive density_ or __elpd__:

$$
\text{elpd} = E(\log p\big(\tilde y_i)\big) = \int \log p(\tilde y_i) \underbrace{f(\tilde y_i)}_\text{true dgp} d \tilde y
$$

Obviously, we do not know the "true" data generating process $f$ and must therefore settle for an approximation. But lets not worry about that for the moment. 

To keep comparability with the given dataset, we  define a measure of predictive accuracy for the $n$ data points taken one at a time: the _expected out-of-sample log pointwise predictive density_.

$$
\text{elppd} = \sum_{i=1}^n E\big(\log p(\tilde y_i)\big)
$$

The advantage of this measure will become clear once we draw a connection between __elppd__ and cross-validation. 


$$
p(\tilde y \mid y ) = p(\tilde y \mid \boldsymbol \theta, y)
$$

_Log predictive density_ (or _log-likelihood_):

The log predictive density has an important role in model comparison because, in the limit of large sample sizes, the model with the highest expected log predictive density will provide more accurate predictions.

$$
 \overbrace{\log p(\tilde y_i)}^\text{log post. pred. dist.} = \log \int p(\tilde y_i \mid \boldsymbol \theta)\ p(\boldsymbol \theta) d \boldsymbol \theta
$$

Why are we only using the likelihood (or data model) and not the prior in this calculation? Because we are interested here in summarizing the fit of model _to data_. The prior is relevant in estimating the parameters but not in assessing a model's accuracy.



HERE

**Brief overview of leave-one-out cross-validation**

Bayesians worry more about paying the cost of spliting the original dataset into training and testing sets.





Suppose you omitted the ith observation, obtained draws from the posterior distribution whose PDF is f(θ∣∣y−i), and evaluated the log-likelihood for the ith observation ℓ(θ;yi) over the posterior draws of θ
You could do that N times, leaving out the ith observation each time, and sum the log-likelihoods
One problem is that doing so would take a lot of time and be computationally unreliable because something would go wrong with at least one of the Markov Chains
But it is possible to estimate what would happen if you were to do all that, using only the posterior draws obtained from conditioning once on the entire dataset with one verifiable assumption that no yi has an outsized influence on the posterior PDF




file:///Users/andrescastroaraujo/Desktop/Dropbox/Columbia/1.%20Fall/Data%20Mining%20for%20the%20Social%20Sciences/r/Resampling_M_and_Trees.html


https://m-clark.github.io/introduction-to-machine-learning/concepts.html#cross-validation


https://projecteuclid.org/download/pdfview_1/euclid.ssu/1268143839

### LOO

```{r, message=FALSE}
b_mod2 <- stan_glm(inc.party.vote ~ growth + I(growth^2), data = hibbs)

library(loo)
loo(b_mod)
loo(b_mod2, k_threshold = 0.7)
compare(loo(b_mod), loo(b_mod2, k_threshold = 0.7))
```






## Multilevel models