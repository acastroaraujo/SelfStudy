---
title: "<strong>mixture models</strong>"
author: "andrés castro araújo"
date: "`r Sys.Date()`"
output: 
  html_document: 
    code_folding: show
    theme: paper
    toc: yes
    toc_float:
      collapsed: yes
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = "", fig.align = "center",
                      fig.width = 5, fig.height = 3)

library(tidyverse)
theme_set(theme_minimal(base_family = "Avenir Next Condensed", base_line_size = 0))
```


```{css, echo=FALSE}
blockquote {
    padding: 10px 20px;
    margin: 0 0 20px;
    font-size: 12px;
    color: #828282;
    border-left: 14px solid #EEE;
}
body {
    font-size: 14px;
}
```

****

Most of this notebook comes straight from from [__Modern Statistics for Modern Biology__](http://web.stanford.edu/class/bios221/book/index.html) [-@holmes2018modern, chap. 4] by Susan Holmes and Wolgang Huber.

****

## introduction

Simple probability distributions can be used as the building blocks for more complex and realistic statistical models.

The following graph shows a mixture of two normal distributions. We call these __finite mixtures__. Other mixtures can involve almost as many distributions as we have observations. These are called __infinite mixtures__. 

```{r}
draw_normal_mixture <- function(N = 1e3, means = c(-1, 1), sds = c(0.5, 0.5)) {
  mix <- sample(c(TRUE, FALSE), N, replace = TRUE, prob = c(0.5, 0.5))
  output <- rnorm(n = length(mix), 
        mean = ifelse(mix, means[[1]], means[[2]]),
        sd = ifelse(mix, sds[[1]], sds[[2]])
        ) 
  
  data.frame(x = output)
}

draw_normal_mixture() %>% 
  ggplot(aes(x)) + 
  geom_histogram(binwidth = 0.1, color = "white", fill = "steelblue")
```

Note that as we increase the number of observations and bins, the histogram gets nearer to a smooth curve. This smooth limiting curve is called the density function of the random variable. 

```{r}
draw_normal_mixture(N = 1e6) %>% 
  ggplot(aes(x, y = ..density..)) + 
  geom_histogram(bins = 500, color = NA, fill = "steelblue")
```

In this case, each of the two normals can be written explicitly as

$$
\phi(x) = \frac{1}{\sigma \sqrt{2 \pi}} \exp \Bigg( - \frac{1}{2} \bigg( 
\frac{x - \mu}{\sigma} \bigg)^2 \Bigg)
$$

And the mixture density in our previous example can simply be written as:

$$
\begin{align}
f(x) = \lambda_1 \ \phi_1(x) +  \lambda_2 \ \phi_2(x), && \underbrace{\lambda_1 + \lambda_2 = 1}_\text{mixing proportions}
\end{align}
$$

****

Suppose that a dataset was generated from a mixture of two normals with the following parameters:

$$
\begin{align}
&\mu_1 = -1 && \mu_2 = 1 \\
&\sigma_1 = 0.5 && \sigma_2 = 0.75 \\
&\lambda_1 = 0.7 && \lambda_2 = 0.3 \\
\end{align}
$$

```{r}
normal_mixture_pdf <- function(x, mu = c(-1, 1), sigma = c(0.5, 0.75), lambda = c(0.7, 0.3)) {
    lambda[[1]] * dnorm(x, mu[[1]], sigma[[1]]) + 
    lambda[[2]] * dnorm(x, mu[[2]], sigma[[2]])
}

ggplot() +
  geom_area(aes(x = -4:4), stat = "function", fun = normal_mixture_pdf, 
    fill = "steelblue") +
  labs(y = "mixture density", x = "x")
```

Here is an example of a dataset generated out of such model.

```{r}
set.seed(123)
N   <- 500
mus <- c(-1, 1)
u   <- sample(1:2, N, replace = TRUE, prob = c(0.7, 0.3))  ## label
sigmas <- c(0.5, 0.75)
x   <- rnorm(N, mean = mus[u], sd = sigmas[u])
(df  <- tibble(u, x))
```

Number of observations in each group:

```{r}
table(df$u)
```

And because we know the labels $u$, we can estimate both means using separate maximum likelihood estimates for each group. The overall MLE is obtained by maximizing the following equation (or it's logarithm):

$$
f(x, u \mid \boldsymbol \theta) = \prod_{\{i: u_i = 1\}} \phi_1(x_i) \prod_{\{i: u_i = 2\}} \phi_2(x_i)
$$

Note that the maximization can be split into two independent pieces and solved as if we had two different MLEs to find. The MLE for the mean and variance of normal distributions are simply:

$$
\hat \mu = \frac{1}{n} \sum_{i = 1}^n x_i \hspace{1cm} \text{and} \hspace{1cm}
\hat \sigma^2 = \frac{1}{n} \sum_{i = 1}^n (x_i - \hat \mu)^2
$$

```{r}
df %>% 
  group_by(u) %>% 
  summarize(mu_hat = mean(x), sd_hat = sd(x), NK = n()) %>% 
  mutate(lambda = NK / sum(NK))
```

The problem, however, is that _we won't know $u_i$ or the mixture proportions_ ( $\lambda_1, \lambda_2$). This is the problem that the __EM algorithm__ is supposed to solve. 

The next section goes more in depth into what this really means. For the time being, we can use the [__`mixtools`__](https://cran.r-project.org/web/packages/mixtools/index.html) package to provide a fast implementation of the algorithm and compare the results.

```{r}
output <- mixtools::normalmixEM(df$x)

output[c("lambda", "mu", "sigma")]
```

## the EM algorithm

The [_expectation-maximization algorithm_](https://en.wikipedia.org/wiki/Expectation–maximization_algorithm) is used to make inferences about hidden groupings (or latent variables) in data. These can be any number $K$ of groupings.

It's a popular procedure that alternates between two steps:

- Pretending we know the probability with which each observation belongs to a component (or cluster) and __estimating the distribution parameters__ of the components. 

- Pretending we know the parameters of the component (or cluster) distributions and __estimating the probability with which each observation belongs to them__. 

    We refer to these probabilities as the _weights_ of each individual data point ($w_{i,k}$).

In other words, we solve a difficult optimization problem by iteratively pretending we know one part of the solution to compute the other part.

For example, suppose we measure a variable $X$ on a series of objects. We also think that these measurements come from $K$ different groups (in this example we assume $K = 2$). We then start by _augmenting_ the data with the unobserved (or missing or latent) group label, which we call $U$. We are now interested in discovering the values of $U$, and also the unknown parameters that describe the underlying densities (e.g. $\mu_1$, $\sigma_1$, $\mu_2$, $\sigma_2$). 

After starting with initial guesses about the cluster parameters and mixing proportions (i.e. $\lambda$), we then proceed to 

1. Use the current parameter guesses to calculate the weights $w_{i,k}$ (__E-step__).

2. Use the current weights to maximize the weighted likelihood and getting new parameter estimates (__M-step__).

>These two iterations (E and M) are repeated until the improvements are small; this is a numerical indication that we are close to a flattening of the likelihood and so we have reached a local maximum. It’s good practice to repeat such a procedure several times from different starting points and check that we always get the same answer.

### implementation

```{r}
EM <- function(x, k) {

  # Initial Guesses -------------------------------------------------
  # There are better ways to do this...
  sigma <- rep(1, k)
  mu <- sample(x, k) 
  lambda <- rep(1 / k, k)
  
  # Convergence is generally detected by computing the value of the log-likelihood
  # after each iteration and halting when it appears not to be changing in a
  # significant manner from one iteration to the next
  
  iter <- 0
  ll_init <- 999
  ll_diff <- 999
  
  while (ll_diff >= 0.00001) {
  
    # E Step ------------------------------------------------------------------
    # Calculate weights (assuming we know lambda, mu, and sigma)
    
    w <- map(1:k, function(k) lambda[[k]] * dnorm(x, mu[[k]], sigma[[k]]))
    denominator <- reduce(w, `+`)
    weights <- do.call(cbind, w) / denominator 
    
    # M Step ------------------------------------------------------------------
    # Calculate lambda, mu, and sigma (assuming we know weights)
    
    NK <- colSums(weights)
    lambda <- NK / nrow(weights)
    mu <- colSums(weights * x) / NK 
    sigma <- purrr::map_dbl(1:k, function(k) sqrt(sum(weights[, k] * (x - mu[[k]])^2) / NK[[k]]))
    
    # Log likelihood ----------------------------------------------------------
    
    ll <- sum(log(denominator))
    
    # Convergence Stuff -------------------------------------------------------
    
    iter <- iter + 1
    ll_diff <- abs(ll - ll_init)
    ll_init <- ll
  
  }
  
  message(iter, " iterations!")
  list(mu = mu, sigma = sigma, lambda = lambda, x = x, posterior = weights, loglikelihood = ll)

}

new_output <- EM(df$x, k = 2)
new_output[c("lambda", "mu", "sigma")]
```

Here is how Holmes and Huber [-@holmes2018modern] describe the usefulness of the EM algorithm:

>It shows us how we can tackle a difficult problem with too many unknowns by alternating between solving simpler problems. In this way, we eventually find estimates of hidden variables.

>It provides a first example of _soft_ averaging i.e., where we don’t decide whether an observation belongs to one group or another, but allow it to participate in several groups by using probabilities of membership as weights, and thus obtain more nuanced estimates.

>The method employed here can be extended to the more general case of __model-averaging__ (Hoeting et al. 1999). It can be sometimes beneficial to consider several models simultaneously if we are unsure which one is relevant for our data. We can combine them together into a weighted model. The weights are provided by the likelihoods of the models.

### identifiability

We should make sure that our probability distributions are [_identifiable_](https://en.wikipedia.org/wiki/Identifiability), "that if we have distinct representations of the model, they make distinct observational claims" [@shalizi2013advanced, chap. 19]. Mixture models can exhibit many issues with identifiability. For example __label degeneracy__, which means that "we can always swap the labels of any two clusters with no effect on anything observable at all --- if we decide that cluster A is now cluster number B and vice versa, that doesn’t change the distribution of $X$ at all" [@shalizi2013advanced].

__Another coin flip example:__

Suppose we have two unfair coins whose probabilities of heads are $p_1 = 0.125$ and $p_2 = 0.25$. We pick coin 1 with probability $\lambda$ (and coin 2 with probability $1-\lambda$). After picking a coin, we toss it twice and record the number of heads $K$.

```{r}
set.seed(911)
coin_simulation <- function(N, p1, p2, lambda) {
  coin <- sample(c(1, 2), size = N, replace = TRUE, prob = c(lambda, 1 - lambda))
  K <- rbinom(n = length(coin), size = 2, prob = ifelse(coin == 1, p1, p2))
  return(K)
}

table(coin_simulation(N = 100, p1 = 1/8, p2 = 1/4, lambda = 1/8))
table(coin_simulation(N = 100, p1 = 1/8, p2 = 1/4, lambda = 1/4))
```

After seeing both contingency tables, can we uniquely estimate the values of $p_1$, $p_2$, and $\lambda$? This seems _very unlikely_.

More generally, the problem of identifiability arises when there are too many degrees of freedom in the parameters.

### zero inflated data

Mixture models are useful whenever we have observations that can be related to different causes. Note that these models can incorporate different probability distributions to model the same outcome $y$. 

>Count variables are especially prone to needing a mixture treatment. The reason is that a count of zero can often arise more than one way. A "zero" means that nothing happened, and nothing can happen either because the rate of events is low or rather because the process that generates events failed to get started [@mcelreath2015statistical].

The zero inflated model will usually look something like this:

$$
f_{ZI}(y) = \lambda \ \delta_0(y) + (1 - \lambda) \ f_{\text{count}}(y)
$$
Here, the $\delta_0$ is Dirac's delta function, which represents a probability distribution that has all its mass at 0. 

## bootstrap

The __sampling distribution__ is the set of possible data sets that could have been observed, if the data collection process had been repeated many many times. The true sampling distribution of a statistic $\hat\tau$ (e.g. mean, variance, etc) is often hard to know as it requires many different data samples. 

Thus, we use the standard error –which is nothing more than the _estimated_ standard deviation of the sampling distribution– to characterize the variability of an estimator.

```{r}
set.seed(1234)
sim <- tibble(dist = replicate(1e5, mean(rnorm(n = 100, mean = 1, sd = 2))))
one_draw <- rnorm(n = 100, mean = 1, sd = 2)

ggplot(sim, aes(x = dist, fill = "sampling distribution")) + 
  geom_density(color = NA) +
  geom_vline(xintercept = mean(one_draw), color = "pink", linetype = "dashed") + 
  scale_fill_manual(values = "steelblue") + 
  labs(x = "x", fill = NULL)
```

A popular alternative to calculating standard errors analytically, is using a computational approximation called the __bootstrap__.

```{r}
B <- 5000
bmeans <- replicate(B, {
  i = sample(100, 100, replace = TRUE)
  mean(one_draw[i])
})

ggplot(NULL) + 
  geom_density(aes(x = sim$dist, fill = "sampling distribution"), color = NA) +
  geom_density(aes(x = bmeans, fill = "bootstrap means"), color = NA, alpha = 0.5) +
  geom_vline(xintercept = mean(one_draw), color = "pink", linetype = "dashed") + 
  scale_fill_manual(values = c("pink", "steelblue")) +
  labs(x = "x", fill = NULL)
```

The so-called non-parametric bootstrap can be conceived as an extreme case of mixture models: _we model our sample of $n$ data points as a mixture of $n$ point masses_.

The the __empirical cumulative distribution function__ (ECDF) for a sample of size $n$ is:

$$
\hat F_n(x) = \sum_{i = n}^n 1_{x \leq x_i}
$$

Now we can now write the _density_ of our sample data as a mixture of $n$ point masses (each one of them with its own delta function $\delta_{x_i}$):

$$
\hat f_n(x) = \sum_{i = n}^n \lambda_i \delta_{x_i}(x) = \frac{1}{n} \sum_{i = n}^n \delta_{x_i}(x) 
$$

This is a lot of notation just to say that the probability mass at each point is simply $\frac{1}{n}$.

>The bootstrap principle approximates the true sampling distribution of $\hat\tau$ by creating new samples drawn from the empirical distribution built from the original sample. We _reuse_ the data (by considering it a mixture distribution of $\delta$s) to create new "datasets" by taking samples and looking at the sampling distribution of the statistics computed on them. This is called the nonparametric bootstrap resampling approach.

## infinite mixtures





Sometimes mixtures can be useful even if we don’t aim to assign a label to each observation or, to put it differently, if we allow as many "labels" as there are observations. If the number of mixture components is as big as (or bigger than) the number of observations, we say we have an __infinite mixture__. Let’s look at some examples.




ADD STUFF FROM BDA

Infinite mixture models are good for constructing new distributions (such as the gamma-Poisson or the Laplace) out of more basic ones (such as binomial, normal, Poisson). Common examples are

mixtures of normals

beta-binomial mixtures

gamma-Poisson

gamma-exponential



## References